Q1. Difference between Hadoop 1 and Hadoop 2?
Ans1. 
1.As Hadoop 1 introduced prior to Hadoop 2 so has some less components and APIs as compare to that of Hadoop 2.
On other hand Hadoop 2 introduced after Hadoop 1 so has more components and APIs as compare to Hadoop 1 such as YARN API,YARN FRAMEWORK, and enhanced Resource Manager.

2.Hadoop 1 only supports MapReduce processing model in its architecture and it does not support non MapReduce tools.
On other hand Hadoop 2 allows to work in MapReducer model as well as other distributed computing models like Spark, Hama, Giraph, Message Passing Interface) MPI & HBase coprocessors.

3.Map reducer in Hadoop 1 is responsible for processing and cluster-resource management.
On other hand in case of Hadoop 2 for cluster resource management YARN is used while processing management is done using different processing models.

4.Hadoop 1 is implemented as it follows the concepts of slots which can be used to run a Map task or a Reduce task only.
On other hand Hadoop 2 follows concepts of containers that can be used to run generic tasks.

Q2.In hadoop 2 why the block size has been set to 128 mb?
Ans2.In Hadoop, input data files are divided into blocks of a prticular size(128 mb by default) and then these blocks of data are stored on different data nodes.
Hadoop is designed to process large volumes of data. Each block’s information(its address ie on which data node it is stored) is placed in namenode. So if the block size is too small, 
then there will be a large no. of blocks to be stored on data nodes as well as a large amount of metadata information needs to be stored on namenode, Also each block of data is processed by a Mapper task. 
If there are large no. of small blocks, a lot of mapper tasks will be required. So having small block size is not very efficient.
Also the block size should not be very large such that , parallelism cant be achieved. It should not be such that the system is waiting a very long time for one unit of data processing to finish its work.
A balance needs to be maintained. That’s why the default block size is 128 MB. It can be changed as well depending on the size of input files.

Q3.Why namenode rely on memory rather than datanode?
Ans3.Name Node only store metadata which is related to the different blocks and because of this reason it needs high memory space. Data Nodes don’t need large memory space.

Q4. Suppose you have 10 PB of data. Metadata is actually store object of file and folder,each obj 200 B. How much min Namenode RAM memory you need for your data node in a cluster to manage the metadata?
Ans4.Estimate minimum Namenode RAM size for HDFS with 10 PB capacity, block size 64 MB, average metadata size for each block is 200 B, replication factor is 3. 
Ans:  10 PB/ (64MB *3) *200B = (10 * 10^15)/(64* 10^6 * 3)*200 B = 10^10/(64*3)* 300B = 1.5625e10 B
